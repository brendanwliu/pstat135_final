{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteorological Data for Bennett's Point, Ace Basin 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = \"./input/raw/ACE Basin/meteorological/data/Bennett's Point/\"\n",
    "\n",
    "aceb04_data = spark.read.csv(data_sample + \"acebpmet2004.txt\",header=True)\n",
    "dropped_cols = [f for f in aceb04_data.columns if f[0] == 'F']\n",
    "aceb04_data = aceb04_data.drop(*dropped_cols)\n",
    "\n",
    "split_col = F.split(aceb04_data['DateTimeStamp'], '\\ ')\n",
    "aceb04_data = aceb04_data.withColumn('SMPLDATE', split_col.getItem(0)).withColumn('SMPLTIME', split_col.getItem(1))\n",
    "aceb04_data = aceb04_data.drop('DateTimeStamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of 2004 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43924, 28)\n"
     ]
    }
   ],
   "source": [
    "print((aceb04_data.count(), len(aceb04_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteorological Data for Bennett's Point, Ace Basin 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "aceb05_data = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(data_sample + \"acebpmet2005.txt\")\n",
    "aceb05_data = aceb05_data.drop(*['STNCODE', 'USRCODES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of 2005 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44034, 28)\n"
     ]
    }
   ],
   "source": [
    "print((aceb05_data.count(), len(aceb05_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteorological Data for Bennett's Point, Ace Basin 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "aceb06_data = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(data_sample + \"acebpmet2006.txt\")\n",
    "aceb06_data = aceb06_data.drop(*['STNCODE', 'USRCODES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of 2006 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39043, 28)\n"
     ]
    }
   ],
   "source": [
    "print((aceb06_data.count(), len(aceb06_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation of ALL Meteorological Data for Bennett's Point, Ace Basin from 2004-2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "acebmet_data = aceb06_data.union(aceb04_data).union(aceb05_data)\n",
    "acebmet_data.toPandas().to_csv(\"input/clean/agg/AceBasinMeteor_2004-2006.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127001, 28)\n"
     ]
    }
   ],
   "source": [
    "print((acebmet_data.count(), len(acebmet_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "acebmet_data.select([count(when(isnan(c), c)).alias(c) for c in acebmet_data.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see there are no missing values in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127001"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acebmet_data.count()\n",
    "acebmet_data.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the number of distinct values is the same as the number of values overall. Thus, we do not have any duplicate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype for automated script to aggregate meterological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "rootdir = './input/raw'\n",
    "\n",
    "file_list = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        file_list.append(os.path.join(subdir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating lists of relevant .txt file names, separated into meteorological, nutrient, and water quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = [file for file in file_list if any(txt in file for txt in ['.txt'])]\n",
    "regex = re.compile(r'.*(Readme).*|.*(checkpoint).*')\n",
    "text_list = [i for i in text if not regex.match(i)]\n",
    "text_list\n",
    "\n",
    "met_list = [file for file in text_list if any(txt in file for txt in ['meteorological'])]\n",
    "nut_list = [file for file in text_list if any(txt in file for txt in ['nutrient'])]\n",
    "water_list = [file for file in text_list if any(txt in file for txt in ['water quality'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Meteorological file names by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_2004 = [file for file in met_list if any(txt in file for txt in ['2004'])]\n",
    "met_2005 = [file for file in met_list if any(txt in file for txt in ['2005'])]\n",
    "met_2006 = [file for file in met_list if any(txt in file for txt in ['2006'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating Meteorological data of ALL locations for 2005 and 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "met05_data = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(met_2005)\n",
    "met05_data = met05_data.drop(*['USRCODES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "met06_data = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(met_2006)\n",
    "met06_data = met06_data.drop(*['USRCODES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met06_data.columns == met05_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+--------+-----+-------+--------+-------+--------+---+-----+------+-----+------+---+-----+------+-----+------+----+----+------+-------+--------+-------+--------+-------+------+-------+-------+--------+\n",
      "|STNCODE|CLASS|SMPLDATE|SMPLTIME|ATemp|MaxTemp|MaxTempT|MinTemp|MinTempT| RH|MaxRH|MaxRHT|MinRH|MinRHT| BP|MaxBP|MaxBPT|MinBP|MinBPT|WSpd|Wdir|SDWDir|MaxWSpd|MaxWSpdT|MinWSpd|MinWSpdT|TotPrcp|TotPAR|AvgVolt|TotSRad|CummRain|\n",
      "+-------+-----+--------+--------+-----+-------+--------+-------+--------+---+-----+------+-----+------+---+-----+------+-----+------+----+----+------+-------+--------+-------+--------+-------+------+-------+-------+--------+\n",
      "|      0|    0|       0|       0|    0|      0|       0|      0|       0|  0|    0|     0|    0|     0|  0|    0|     0|    0|     0|   0|   0|     0|      0|       0|      0|       0|      0|     0|      0|      0|       0|\n",
      "+-------+-----+--------+--------+-----+-------+--------+-------+--------+---+-----+------+-----+------+---+-----+------+-----+------+----+----+------+-------+--------+-------+--------+-------+------+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "met05_data.select([count(when(isnan(c), c)).alias(c) for c in met05_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+--------+-----+-------+--------+-------+--------+---+-----+------+-----+------+---+-----+------+-----+------+----+----+------+-------+--------+-------+--------+-------+------+-------+-------+\n",
      "|STNCODE|CLASS|SMPLDATE|SMPLTIME|ATemp|MaxTemp|MaxTempT|MinTemp|MinTempT| RH|MaxRH|MaxRHT|MinRH|MinRHT| BP|MaxBP|MaxBPT|MinBP|MinBPT|WSpd|Wdir|SDWDir|MaxWSpd|MaxWSpdT|MinWSpd|MinWSpdT|TotPrcp|TotPAR|AvgVolt|DnIrrad|\n",
      "+-------+-----+--------+--------+-----+-------+--------+-------+--------+---+-----+------+-----+------+---+-----+------+-----+------+----+----+------+-------+--------+-------+--------+-------+------+-------+-------+\n",
      "|      0|    0|       0|       0|    0|      0|       0|      0|       0|  0|    0|     0|    0|     0|  0|    0|     0|    0|     0|   0|   0|     0|      0|       0|      0|       0|      0|     0|      0|      0|\n",
      "+-------+-----+--------+--------+-----+-------+--------+-------+--------+---+-----+------+-----+------+---+-----+------+-----+------+----+----+------+-------+--------+-------+--------+-------+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "met06_data.select([count(when(isnan(c), c)).alias(c) for c in met06_data.columns]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
